#!/usr/bin/env python3
"""
TTD-DR Research Planner - Test Data & Demonstration
Task 1.2.2 - Test Data Generation and Real-world Examples

Comprehensive test data, example queries, and demonstration scenarios
for the Research Planner agent.
"""

import json
from research_planner_agent import ResearchPlannerAgent

# Test case categories
TEST_CATEGORIES = {
    'basic_queries': [
        "AIã¨ã¯ä½•ã‹",
        "æ©Ÿæ¢°å­¦ç¿’ã®åŸºæœ¬æ¦‚å¿µ",
        "æ·±å±¤å­¦ç¿’ã®æ¦‚è¦",
        "è‡ªç„¶è¨€èªå‡¦ç†ã«ã¤ã„ã¦",
        "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã®åŸºç¤"
    ],
    
    'technical_surveys': [
        "Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æŠ€è¡“çš„è©³ç´°ã¨å®Ÿè£…æ–¹æ³•",
        "åˆ†æ•£æ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã®è¨­è¨ˆåŸç†ã¨ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£",
        "ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯ã¨æœ€é©åŒ–æ‰‹æ³•",
        "ã‚¨ãƒƒã‚¸AIãƒ‡ãƒã‚¤ã‚¹ã®å‡¦ç†èƒ½åŠ›å‘ä¸ŠæŠ€è¡“",
        "é‡å­æ©Ÿæ¢°å­¦ç¿’ã®ç¾çŠ¶ã¨æŠ€è¡“çš„èª²é¡Œ"
    ],
    
    'comparative_analyses': [
        "GPT-4ã€Claude 3.5ã€Gemini Proã®æ€§èƒ½ãƒ»æ©Ÿèƒ½ãƒ»ã‚³ã‚¹ãƒˆæ¯”è¼ƒ",
        "TensorFlow vs PyTorch: æ·±å±¤å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯è©³ç´°æ¯”è¼ƒ",
        "AWSã¨Google Cloudã®æ©Ÿæ¢°å­¦ç¿’ã‚µãƒ¼ãƒ“ã‚¹æ¯”è¼ƒåˆ†æ",
        "BERTã€RoBERTaã€DeBERTaã®è¨€èªç†è§£æ€§èƒ½æ¯”è¼ƒ",
        "è‡ªå‹•é‹è»¢æŠ€è¡“: Tesla vs Waymo vs Cruiseã®æŠ€è¡“æˆ¦ç•¥æ¯”è¼ƒ"
    ],
    
    'implementation_studies': [
        "å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®åŠ¹ç‡çš„ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè£…",
        "ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”»åƒèªè­˜ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰æ–¹æ³•",
        "åˆ†æ•£æ·±å±¤å­¦ç¿’ã®å®Ÿè£…ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ",
        "ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚·ã‚¹ãƒ†ãƒ ã®ä¼æ¥­å°å…¥å®Ÿè£…ã‚¬ã‚¤ãƒ‰",
        "MLOpsãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰ã¨é‹ç”¨è‡ªå‹•åŒ–"
    ],
    
    'academic_research': [
        "èª¬æ˜å¯èƒ½AI(XAI)ã®ç†è«–çš„åŸºç›¤ã¨è©•ä¾¡æ‰‹æ³•ã®åŒ…æ‹¬çš„èª¿æŸ»",
        "é€£åˆå­¦ç¿’ã«ãŠã‘ã‚‹ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·æŠ€è¡“ã®æœ€æ–°ç ”ç©¶å‹•å‘",
        "å¤šè¨€èªå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®è¨€èªé–“è»¢ç§»å­¦ç¿’æ©Ÿæ§‹ã®è§£æ",
        "ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ»ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯AIã®çµ±åˆã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®ç†è«–çš„è€ƒå¯Ÿ",
        "äººå·¥çŸ¥èƒ½ã®å€«ç†çš„èª²é¡Œã¨è¦åˆ¶ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®å›½éš›æ¯”è¼ƒç ”ç©¶"
    ],
    
    'industry_reports': [
        "2024å¹´AIå¸‚å ´å‹•å‘ã¨ä¸»è¦ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®æˆ¦ç•¥åˆ†æ",
        "ä¼æ¥­ã®AIå°å…¥çŠ¶æ³ã¨æŠ•è³‡å¯¾åŠ¹æœã®å®Ÿæ…‹èª¿æŸ»",
        "AIäººæã®éœ€çµ¦çŠ¶æ³ã¨äººæè‚²æˆã®èª²é¡Œåˆ†æ",
        "è‡ªå‹•é‹è»¢æ¥­ç•Œã®å¸‚å ´äºˆæ¸¬ã¨æŠ€è¡“ç«¶äº‰ã®ç¾çŠ¶",
        "AIåŠå°ä½“å¸‚å ´ã®æˆé•·äºˆæ¸¬ã¨ç«¶åˆåˆ†æ"
    ]
}

# Constraint templates for different use cases
CONSTRAINT_TEMPLATES = {
    'quick_overview': {
        'target_length': 2000,
        'max_sections': 4,
        'search_iterations': 8
    },
    
    'standard_research': {
        'target_length': 5000,
        'max_sections': 6,
        'search_iterations': 12
    },
    
    'comprehensive_study': {
        'target_length': 8000,
        'max_sections': 8,
        'search_iterations': 15
    },
    
    'academic_paper': {
        'target_length': 10000,
        'max_sections': 10,
        'search_iterations': 18,
        'academic_level': 'graduate',
        'citation_style': 'APA'
    },
    
    'industry_report': {
        'target_length': 6000,
        'max_sections': 7,
        'search_iterations': 12,
        'languages': ['ja', 'en']
    }
}

# Domain context templates
DOMAIN_CONTEXTS = {
    'ai_ml': {
        'field': 'AI/ML',
        'keywords': ['äººå·¥çŸ¥èƒ½', 'æ©Ÿæ¢°å­¦ç¿’', 'æ·±å±¤å­¦ç¿’', 'ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯'],
        'excluded_topics': []
    },
    
    'nlp': {
        'field': 'Natural Language Processing',
        'keywords': ['è‡ªç„¶è¨€èªå‡¦ç†', 'NLP', 'è¨€èªãƒ¢ãƒ‡ãƒ«', 'Transformer'],
        'excluded_topics': ['éŸ³å£°èªè­˜', 'ç”»åƒå‡¦ç†']
    },
    
    'computer_vision': {
        'field': 'Computer Vision',
        'keywords': ['ç”»åƒèªè­˜', 'ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³', 'CNN', 'ç‰©ä½“æ¤œå‡º'],
        'excluded_topics': ['éŸ³å£°', 'ãƒ†ã‚­ã‚¹ãƒˆ']
    },
    
    'robotics': {
        'field': 'Robotics',
        'keywords': ['ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹', 'è‡ªå‹•åŒ–', 'åˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ ', 'ã‚»ãƒ³ã‚µãƒ¼'],
        'excluded_topics': []
    },
    
    'autonomous_systems': {
        'field': 'Autonomous Systems',
        'keywords': ['è‡ªå‹•é‹è»¢', 'è‡ªå¾‹ã‚·ã‚¹ãƒ†ãƒ ', 'ã‚»ãƒ³ã‚µãƒ¼ãƒ•ãƒ¥ãƒ¼ã‚¸ãƒ§ãƒ³'],
        'excluded_topics': []
    }
}

# User preference templates
USER_PREFERENCES = {
    'evidence_focused': {
        'evidence_weight': 0.9,
        'creativity_level': 0.3,
        'technical_depth': 0.8
    },
    
    'creative_exploration': {
        'evidence_weight': 0.6,
        'creativity_level': 0.8,
        'technical_depth': 0.6
    },
    
    'technical_deep_dive': {
        'evidence_weight': 0.8,
        'creativity_level': 0.4,
        'technical_depth': 0.9
    },
    
    'balanced_approach': {
        'evidence_weight': 0.7,
        'creativity_level': 0.5,
        'technical_depth': 0.7
    }
}


def generate_test_data():
    """Generate comprehensive test data with expected outcomes"""
    
    agent = ResearchPlannerAgent()
    test_data = {}
    
    print("Generating test data for Research Planner...")
    
    for category, queries in TEST_CATEGORIES.items():
        print(f"Processing {category}...")
        test_data[category] = []
        
        for i, query in enumerate(queries):
            # Select appropriate constraints based on category
            if category == 'academic_research':
                constraints = CONSTRAINT_TEMPLATES['academic_paper']
                preferences = USER_PREFERENCES['evidence_focused']
            elif category == 'industry_reports':
                constraints = CONSTRAINT_TEMPLATES['industry_report']
                preferences = USER_PREFERENCES['balanced_approach']
            elif category == 'basic_queries':
                constraints = CONSTRAINT_TEMPLATES['quick_overview']
                preferences = USER_PREFERENCES['balanced_approach']
            else:
                constraints = CONSTRAINT_TEMPLATES['standard_research']
                preferences = USER_PREFERENCES['technical_deep_dive']
            
            # Select domain context
            if 'NLP' in query or 'è‡ªç„¶è¨€èª' in query or 'Transformer' in query:
                domain_context = DOMAIN_CONTEXTS['nlp']
            elif 'ç”»åƒ' in query or 'ãƒ“ã‚¸ãƒ§ãƒ³' in query or 'CV' in query:
                domain_context = DOMAIN_CONTEXTS['computer_vision']
            elif 'ãƒ­ãƒœãƒƒãƒˆ' in query or 'è‡ªå‹•é‹è»¢' in query:
                domain_context = DOMAIN_CONTEXTS['autonomous_systems']
            else:
                domain_context = DOMAIN_CONTEXTS['ai_ml']
            
            # Generate plan
            result = agent.plan_research(query, constraints, domain_context, preferences)
            
            test_case = {
                'id': f"{category}_{i+1:02d}",
                'query': query,
                'constraints': constraints,
                'domain_context': domain_context,
                'user_preferences': preferences,
                'result': result,
                'timestamp': None  # Would be filled in real implementation
            }
            
            test_data[category].append(test_case)
    
    return test_data


def demonstrate_research_planner():
    """Demonstrate Research Planner capabilities with real examples"""
    
    agent = ResearchPlannerAgent()
    
    print("=== TTD-DR Research Planner Demonstration ===\n")
    
    # Demonstration 1: Basic Query
    print("ğŸ“‹ Demo 1: Basic Technical Query")
    print("Query: 'Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æŠ€è¡“è©³ç´°'")
    
    result1 = agent.plan_research("Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æŠ€è¡“è©³ç´°")
    
    if result1['status'] == 'success':
        plan1 = result1['plan']
        print(f"âœ… Generated plan with {plan1['structure_plan']['section_count']} sections")
        print(f"ğŸ“Š Complexity Score: {plan1['plan_metadata']['complexity_score']}")
        print(f"â±ï¸  Estimated Duration: {plan1['plan_metadata']['estimated_duration']} minutes")
        print(f"ğŸ” Search Iterations: {plan1['search_strategy']['total_iterations']}")
        print(f"ğŸ“ˆ Quality Score: {result1['validation']['scores']['overall_score']:.3f}")
        
        print("\nğŸ“‘ Section Structure:")
        for i, section in enumerate(plan1['structure_plan']['sections'], 1):
            print(f"  {i}. {section['title']} ({section['target_length']} words)")
    
    print("\n" + "="*60 + "\n")
    
    # Demonstration 2: Comparative Analysis
    print("ğŸ“‹ Demo 2: Comparative Analysis")
    print("Query: 'GPT-4ã¨Claude 3.5ã®è©³ç´°æ€§èƒ½æ¯”è¼ƒ'")
    
    constraints2 = {'target_length': 6000, 'max_sections': 7}
    result2 = agent.plan_research(
        "GPT-4ã¨Claude 3.5ã®è©³ç´°æ€§èƒ½æ¯”è¼ƒ", 
        constraints=constraints2
    )
    
    if result2['status'] == 'success':
        plan2 = result2['plan']
        print(f"âœ… Generated comparative analysis plan")
        print(f"ğŸ“Š Main Question: {plan2['research_objective']['main_question']}")
        print(f"ğŸ¯ Success Criteria:")
        for criterion in plan2['research_objective']['success_criteria']:
            print(f"   â€¢ {criterion}")
        
        print(f"\nğŸ” Search Strategy ({len(plan2['search_strategy']['search_phases'])} phases):")
        for phase in plan2['search_strategy']['search_phases']:
            print(f"   Phase: {phase['phase_name']} ({phase['iteration_count']} iterations)")
    
    print("\n" + "="*60 + "\n")
    
    # Demonstration 3: Academic Research
    print("ğŸ“‹ Demo 3: Academic Research Scenario")
    print("Query: 'èª¬æ˜å¯èƒ½AIã®ç†è«–çš„åŸºç›¤ã¨è©•ä¾¡æ‰‹æ³•ã®åŒ…æ‹¬çš„èª¿æŸ»'")
    
    academic_constraints = CONSTRAINT_TEMPLATES['academic_paper']
    academic_preferences = USER_PREFERENCES['evidence_focused']
    academic_domain = DOMAIN_CONTEXTS['ai_ml']
    
    result3 = agent.plan_research(
        "èª¬æ˜å¯èƒ½AIã®ç†è«–çš„åŸºç›¤ã¨è©•ä¾¡æ‰‹æ³•ã®åŒ…æ‹¬çš„èª¿æŸ»",
        constraints=academic_constraints,
        domain_context=academic_domain,
        user_preferences=academic_preferences
    )
    
    if result3['status'] == 'success':
        plan3 = result3['plan']
        print(f"âœ… Generated academic research plan")
        print(f"ğŸ“š Evidence Requirements:")
        reqs = plan3['search_strategy']['source_requirements']
        print(f"   â€¢ Academic Sources: {reqs['academic_sources']}")
        print(f"   â€¢ Recent Sources: {reqs['recent_sources']}")
        print(f"   â€¢ Primary Sources: {reqs['primary_sources']}")
        
        print(f"ğŸ“‹ Quality Standards:")
        standards = plan3['quality_criteria']['evidence_standards']
        print(f"   â€¢ Citation Density: {standards['citation_density']:.1f} per 1000 words")
        print(f"   â€¢ Source Reliability: {standards['source_reliability']:.2f}")
        print(f"   â€¢ Fact Verification: {standards['fact_verification']}")
    
    print("\n" + "="*60 + "\n")
    
    # Demonstration 4: Error Handling
    print("ğŸ“‹ Demo 4: Error Handling & Edge Cases")
    
    # Test with minimal constraints
    minimal_result = agent.plan_research(
        "AIæ¦‚è¦", 
        constraints={'target_length': 500, 'max_sections': 2}
    )
    
    print(f"Minimal constraints result: {minimal_result['status']}")
    if minimal_result['status'] == 'success':
        minimal_plan = minimal_result['plan']
        print(f"  Sections: {minimal_plan['structure_plan']['section_count']}")
        print(f"  Length: {minimal_plan['structure_plan']['report_length']} words")
    
    # Test with excessive constraints
    excessive_result = agent.plan_research(
        "è¤‡é›‘ãªAIåˆ†æ",
        constraints={'target_length': 15000, 'search_iterations': 40}
    )
    
    print(f"Excessive constraints result: {excessive_result['status']}")
    if excessive_result['status'] == 'success':
        excessive_plan = excessive_result['plan']
        print(f"  Refined iterations: {excessive_plan['search_strategy']['total_iterations']}")
        
    print("\n=== Demonstration Complete ===")


def run_performance_benchmarks():
    """Run comprehensive performance benchmarks"""
    
    import time
    
    agent = ResearchPlannerAgent()
    
    print("=== Performance Benchmarks ===\n")
    
    # Benchmark 1: Query Processing Speed
    print("ğŸ“Š Benchmark 1: Query Processing Speed")
    
    test_queries = [
        "AIåŸºæœ¬æ¦‚å¿µ",
        "æ©Ÿæ¢°å­¦ç¿’æŠ€è¡“ã®è©³ç´°åˆ†æã¨å®Ÿè£…æ–¹æ³•",
        "æ·±å±¤å­¦ç¿’ã¨Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®åŒ…æ‹¬çš„æŠ€è¡“èª¿æŸ»ãŠã‚ˆã³æ€§èƒ½æ¯”è¼ƒåˆ†æ"
    ]
    
    for i, query in enumerate(test_queries, 1):
        start_time = time.time()
        result = agent.plan_research(query)
        end_time = time.time()
        
        processing_time = end_time - start_time
        complexity = result['plan']['plan_metadata']['complexity_score'] if result['status'] == 'success' else 0
        
        print(f"  Query {i} (complexity {complexity:.1f}): {processing_time:.4f}s")
    
    # Benchmark 2: Constraint Handling Speed
    print("\nğŸ“Š Benchmark 2: Constraint Handling Performance")
    
    constraint_scenarios = [
        {'target_length': 1000, 'max_sections': 3},
        {'target_length': 5000, 'max_sections': 6},
        {'target_length': 10000, 'max_sections': 10}
    ]
    
    base_query = "AIæŠ€è¡“ã®åŒ…æ‹¬çš„èª¿æŸ»"
    
    for i, constraints in enumerate(constraint_scenarios, 1):
        start_time = time.time()
        result = agent.plan_research(base_query, constraints=constraints)
        end_time = time.time()
        
        processing_time = end_time - start_time
        sections = result['plan']['structure_plan']['section_count'] if result['status'] == 'success' else 0
        
        print(f"  Scenario {i} ({sections} sections): {processing_time:.4f}s")
    
    # Benchmark 3: Validation Performance
    print("\nğŸ“Š Benchmark 3: Validation Performance")
    
    validation_queries = [
        "ã‚·ãƒ³ãƒ—ãƒ«ãªAIèª¿æŸ»",
        "è¤‡é›‘ãªå¤šæ¬¡å…ƒæŠ€è¡“åˆ†æã‚·ã‚¹ãƒ†ãƒ ã®åŒ…æ‹¬çš„å®Ÿè£…",
        "ã‚°ãƒ­ãƒ¼ãƒãƒ«AIå€«ç†ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®å›½éš›æ¯”è¼ƒ"
    ]
    
    total_validation_time = 0
    
    for query in validation_queries:
        start_time = time.time()
        result = agent.plan_research(query)
        end_time = time.time()
        
        processing_time = end_time - start_time
        total_validation_time += processing_time
        
        if result['status'] == 'success':
            validation_score = result['validation']['scores']['overall_score']
            print(f"  Query (score {validation_score:.3f}): {processing_time:.4f}s")
    
    avg_validation_time = total_validation_time / len(validation_queries)
    print(f"  Average validation time: {avg_validation_time:.4f}s")
    
    print("\n=== Benchmark Complete ===")


def export_sample_plans():
    """Export sample research plans for documentation"""
    
    agent = ResearchPlannerAgent()
    
    sample_queries = [
        {
            'name': 'basic_ai_overview',
            'query': 'AIæŠ€è¡“ã®åŸºæœ¬æ¦‚å¿µã¨ç¾çŠ¶',
            'constraints': CONSTRAINT_TEMPLATES['quick_overview']
        },
        {
            'name': 'technical_survey',
            'query': 'Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æŠ€è¡“è©³ç´°ã¨å®Ÿè£…',
            'constraints': CONSTRAINT_TEMPLATES['standard_research'],
            'domain': DOMAIN_CONTEXTS['nlp']
        },
        {
            'name': 'comparative_analysis',
            'query': 'GPT-4ã¨Claude 3.5ã®è©³ç´°æ¯”è¼ƒåˆ†æ',
            'constraints': CONSTRAINT_TEMPLATES['comprehensive_study']
        },
        {
            'name': 'academic_research',
            'query': 'èª¬æ˜å¯èƒ½AIã®ç†è«–çš„åŸºç›¤ã¨è©•ä¾¡æ‰‹æ³•',
            'constraints': CONSTRAINT_TEMPLATES['academic_paper'],
            'preferences': USER_PREFERENCES['evidence_focused']
        }
    ]
    
    for sample in sample_queries:
        result = agent.plan_research(
            sample['query'],
            constraints=sample.get('constraints'),
            domain_context=sample.get('domain'),
            user_preferences=sample.get('preferences')
        )
        
        if result['status'] == 'success':
            filename = f"sample_plan_{sample['name']}.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(result['plan'], f, ensure_ascii=False, indent=2)
            
            print(f"âœ… Exported {filename}")


if __name__ == "__main__":
    print("TTD-DR Research Planner - Test Data & Demonstration\n")
    
    # Run demonstration
    demonstrate_research_planner()
    
    print("\n" + "="*80 + "\n")
    
    # Run performance benchmarks
    run_performance_benchmarks()
    
    print("\n" + "="*80 + "\n")
    
    # Export sample plans
    print("ğŸ“¤ Exporting Sample Plans...")
    export_sample_plans()
    
    print("\nâœ… Test data generation and demonstration complete!")